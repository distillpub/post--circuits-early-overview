<html>
<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <meta charset="utf-8" />
  <link rel="stylesheet" type="text/css" href="index.css">
  <title>An Overview of Early Vision in InceptionV1</title>
</head>

<d-front-matter>
    <script type="text/json">{{
      "title": "",
      "description": "",
      "password": "circuits",
      "authors": [
        {{
          "author": "Authors TBD",
          "authorURL": "",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        }}
      ]
    }}</script>
  </d-front-matter>

<d-title>
<h1 style='font-size: 240%; grid-column-start: text; grid-column-end: page;'>An Overview of Early Vision in InceptionV1</h1>
<p style='font-size:  150%; line-height: 140%; margin-top: 10px;'>A guided tour of the first five layers of InceptionV1,<br> taxonomized into "neuron groups."</p>
</d-title>
<d-byline></d-byline>

<d-article>

  <section id="thread-nav" class="thread-info" style="grid-column-start: text; grid-column-end: text; margin-top: 20px; margin-bottom: 80px;">
      <div class="icon-multiple-pages"></div>
      <p class="explanation">
          This article is part of the <a style="border-bottom: none; color: #2e6db7; margin-left: 0px;" href="https://distill.pub/2020/circuits/">Circuits thread</a>, a collection of short articles and commentary by an open scientific collaboration delving into the inner workings of neural networks.<br>
          <!--<a style="border-bottom: none; color: #2e6db7; margin-left: 0px;">ðŸ”¬Learn how to get involved.</a>-->
          <!--<a class="overview" href="#">Thread Overview</a>-->
      </p>

      <a class="prev" href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a>
      <div class="next" href="#" style="color:#999;">Curve Detectors
        <div style="color:#BBB; font-size: 90%; line-height: 140%; margin-top: 4px;">Under discussion in <a href="http://slack.distill.pub/">Distill slack</a> <code>#circuits</code></div>
      </div>
  </section>

<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#cards">Playing Cards with Neurons</a></div>
    <div>Layer Overviews</div>
    <ul>
      <li><a href="#conv2d0"><code>conv2d0</code></a></li>
      <li><a href="#conv2d1"><code>conv2d1</code></a></li>
      <li><a href="#conv2d2"><code>conv2d2</code></a></li>
      <li><a href="#mixed3a"><code>mixed3a</code></a></li>
      <li><a href="#mixed3b"><code>mixed3b</code></a></li>
    </ul>
    <!--<div><a href="#open"><code>Open Questions</code></a></div>-->
  </nav>
</d-contents>

<div>

<p>
  The first few articles of the Circuits project will be focused on early vision in InceptionV1
  -- for our purposes, the five convolutional layers leading up to the third pooling layer.
</p>

<figure style='image-rendering: pixelated; grid-column-end: page'>
  {images/Early}
</figure>

<p>
  Over the course of these layers, we see the network go from raw pixels
  up to sophisticated boundary detection, basic shape detection (eg. curves, circles, spirals, triangles),
  and even crude detectors for very small heads.
  Along the way, we see a variety of interesting intermediate features,
  including some of the classic "complex cell" features of neuroscience.
</p>

<p>
  Studying early vision has two major advantages as a starting point in our investigation.
  Firstly, it's particularly easy to study:
  it's close to the input, the circuits are only a few layers deep, there aren't that many different neurons, and the features seem quite simple.
  Secondly, early vision seems most likely to be universal: to have the same features and circuits form across different architectures and tasks.
</p>

<p>
  Before we dive into details explorations of different parts of early vision, we wanted to give a broader overview of how we presently understand it.
  This article sketches out our understanding, as an annotated collection of what we call "neuron groups."
</p>

<p>
  By limiting ourselves to early vision, this article "only" considers the first 1,056 neurons of InceptionV1.
  But our experience is that a thousand neurons is more than enough to be disorienting when one begins studying a model.
  Our hope is that this article will help readers avoid this disorientation by providing some structure and handholds for thinking about them.
</p>

</div>


<h2 id="cards">Playing Cards with Neurons</h2>

<p>
  Dmitri Mendeleev is often accounted to have discovered the Periodic Table by playing "chemical solitaire," writing the details of each element on a card and patiently fiddling with different ways of classifying and organizing them.
  Some modern historians are skeptical about the cards, but Mendeleev's story is a compelling demonstration of that there can be a lot of value in simply organizing phenomena, even when you don't have a theory or firm justification for that organization yet.
  Mendeleev is far from unique in this.
  For example, in biology, taxonomies of species proceeded genetics and theory of evolution giving them a theoretical foundation.
</p>

<p>
  Our experience is that there are suspicious patterns in the neurons of vision models.
  It's not unusual to see a dozen neurons detecting the same feature in different orientations or colors.
  Perhaps even more strikingly, the same "neuron families" seem to recur across models!
  Of course, it's well known that Gabor filters and color contrast detectors reliably comprise neurons in the first layer of convolutional neural networks, but we were quite surprised to see this generalize to later layers.
</p>

<p>
  This article shares our working categorization of units in the first four layers of InceptionV1 into neuron families.
  We've found these helpful for communicating among ourselves and breaking the problem of understanding InceptionV1 into smaller chunks.
  While there are some families we suspect are "real", many others are categories of convenience, or categories we have low-confidence about.
  The main goal of these families is to help researchers orient themselves.
</p>

<h3>Caveats</h2>

<ul>
  <li>This is a broad overview and our understanding of many of these units is low-confidence.</li>
  <li>Many neuron groups are catch-all categories or convenient organizational categories that we don't think reflect fundamental structure.</li>
  <li>Even for neuron groups we suspect do exist in a more fundamental sense (eg. some can be recovered from factorizing the layer's weight matrices) the boundaries of these groups can be blurry and some neurons inclusion involve judgement calls.</li>
</ul>


<br><br>
<hr style="margin-top: 100px; margin-bottom: 100px;">
<br><br>

<h2 id="conv2d0"><code>conv2d0</code></h2>

<p>
  The first conv layer of every vision model we've looked at is mostly comprised of two kinds of features: color-contrast detectors and Gabor filters.
  InceptionV1's <code>conv2d0</code> is no exception to this rule, and most of its units fall into these categories.
</p>

<p>
  With that said, its features are surprisingly messy, compared to the
   perfect color contrast and Gabors we see in other models.
  Several neurons seem to be close to dead.
  We have no way of knowing, but the gradient probably wasn't reaching the early layers very well.
  Note that InceptionV1 predated the adoption of modern techniques like batch norm and Adam, which make it much easier to train deep models well.
</p>

{conv2d0}

<p>
  One subtlety that's worth noting here is that Gabor filters almost always come in pairs of weights which are negative versions of each other, both in InceptionV1 and other vision models.
  A single Gabor filter can only detect edges at some offsets, but the negative version fills in holes, allowing for the formation of complex Gabor filters in the next layer.
</p>

<br><br>
<hr style="margin-top: 100px; margin-bottom: 100px;">
<br><br>

<h2 id="conv2d1"><code>conv2d1</code></h2>

<p>
  In <code>conv2d1</code>, we begin to see some of the classic <a href="https://en.wikipedia.org/wiki/Complex_cell">complex cell</a> features of visual neuroscience.
  These neurons respond to similar patterns to units in <code>conv2d0</code>, but are invariant to some changes in position and orientation.
</p>

<p>
  A nice example of this is the "Complex Gabor" features.
  Like simple Gabor filters, these features detect edges.
  But unlike simple Gabors, they are relatively invariant to the exact position of the edge or which side is dark and which side is white.
  This is achieved by being excited by several Gabor filters in the same orientation -- and most critically, by being excited by Gabor "reciprocal Gabor filters" that detect the same pattern with dark and light switched.
  This can be seen as an early example of the "union over cases" motif.
</p>

<figure style='grid-column-start: page-start; image-rendering: pixelated;'>
  {images/ComplexGabors}
</figure>

<p>
  In addition to Complex Gabors, we see a variety of other features, including
  more invariant color contrast detectors, Gabor-like features that are less selective for a single orientation, and lower-frequency features.
</p>

{conv2d1}

<br>
<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="conv2d2"><code>conv2d2</code></h2>

<p>
In <code>conv2d2</code> we see the emergence of very simple shape predecessors.
This layer sees the first units that might be described as "line detectors", preferring a single longer line to a Gabor pattern and accounting for about 25% of units.
We also see tiny curve detectors, corner detectors, divergence detectors, and a single very tiny circle detector.
One fun aspect of these features is that you can see that they are assembled from Gabor detectors in the feature visualizations, with curves being built from small piecewise Gabor segments.
All of these units still moderately fire in response to incomplete versions of their feature, such as a small curve running tangent to the edge detector.
</p>

<p>
Since <code>conv2d2</code> is a 3x3 convolution, our understanding of these shape precursor features (and some texture features) maps to particular ways Gabor and lower-frequency edges are being spatially assembled into new features.
At a high-level, we see a few primary patterns:
</p>

<figure style='grid-column-start: page-start; image-rendering: pixelated;'>
  {images/GaborCombinations}
</figure>

<p>
  We also begin to see various kinds of texture and color detectors start to become a major constituent of the layer, including color-contrast and color center surround features, as well as Gabor-like, hatch, low-frequency and high-frequency textures.
  A handful of units look for different textures on different sides of their receptive field.
</p>

{conv2d2}





<br>
<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="mixed3a"><code>mixed3a</code></h2>

<p>
<code>mixed3a</code> has a significant increase in the diversity of features we observe.
Some of them  -- curve detectors and high-low frequency detectors --
were discussed in <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In</a>
and will be discussed again in later articles in great detail.
But there are some really interesting circuits in <code>mixed3a</code> which we haven't discussed before,
and we'll go through a couple selected ones to give a flavor of what happens at this layer.
</p>


<p>
<b>Black & White Detectors:</b> One interesting property of <code>mixed3a</code> is the emergence of "black and  white" detectors, which detect the absence of color.
Prior to this <code>mixed3a</code> color contrast detectors look for transitions of a color to near complementary colors (eg. blue vs yellow).
From this layer on, we'll often see color detectors which compare a color to the absence of color.
Additionally, black and white detectors can allow the detection of greyscale images, which may be correlated with ImageNet categories (see <a href="https://storage.googleapis.com/clarity-public/colah/experiments/aprox_weights_1/mixed4a_479.html">4a:479</a> which builds on these features to detect black and white portraits of people).
The circuit for our black and white detector is quite simple:
almost all of its large weights are negative, detecting the absence of colors.
</p>

<figure style='image-rendering: pixelated;'>
  {images/BW}
</figure>

<p>
<b>Small Circle Detector:</b> We also see somewhat more complex shapes in <code>mixed3a</code>. Of course, curves (which we discussed in <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In</a>) are a prominent example of this.
But there's lots of other interesting examples.
For instance, we see a variety of circle detectors and eye detectors form by piecing together early curves and and an early circle detector:
</p>

<figure style='grid-column-start: text-start; image-rendering: pixelated;'>
  {images/SmallCircle}
</figure>

<p>
  <b>Triangle Detectors:</b> While on the matter of basic shapes, we also see triangle detectors form from edges.
  (However, in practice, they seem to often just be used as multi-edge detectors downstream.)
</p>

<figure style='grid-column-start: page-start; image-rendering: pixelated;'>
  {images/Triangle}
</figure>

<p>
  These selected circuits only scratch the surface of the intricate structure in <code>mixed3a</code>.
  Below, we provide a taxonomized overview of all of them:
</p>

{mixed3a}




<br>
<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="mixed3b"><code>mixed3b</code></h2>

<p>
<code>mixed3b</code> straddles two levels of abstraction.
On the one hand, it has some quite sophisticated features that don't really seem like they should be characterized as "early" or "low-level": object boundary detectors, early head detectors, more sophisticated part of shape detectors.
On the other hand, it also has many units that still feel quite low-level, such as color center-surround units.
</p>


<p>
  <b>Boundary detectors:</b> One of the most striking transitions in <code>mixed3b</code> is the formation of boundary detectors.
  When you first look at the feature visualizations and dataset examples,
  you might think these are just another iteration of edge or curve detectors.
  But they are in fact combining a variety of cues to detect boundaries and transitions between objects.
  Perhaps the most important one is the high-low frequency detectors we saw develop at the previous layer.
  Notice that it largely doesn't care which direction in color or frequency is, just that there's a change.
</p>


<figure style='image-rendering: pixelated;'>
  {images/Boundary}
</figure>

<p>
  We sometimes find it useful to think about the "goal" of early vision.
  Gradient descent will only create features if they are useful for features in later layers.
  Which later features incentivized the creation of the features we see in early vision?
  The primary function of high-low frequency detectors seems to be supporting the creation of these boundary detectors.
</p>

<p>
  <b>Curve-based Features:</b>
  Another major theme in this layer is the emergence of more complex and specific shape detectors, base on curves.
  These include more sophisticated curves, circles, loops, S-shapes, spirals, and "evolutes" (curves facing away from the middle).
  We'll discuss these in detail in a forthcoming article on curve circuits, but they warrant mention here.
</p>

<p>
  Conceptually, you can think of the weights as piecing the curve detectors as something like this:
</p>

<figure style='image-rendering: pixelated;'>
  {images/CurveBased}
</figure>

<p>
  <b>Fur detectors:</b> Another interesting (albeit, probably quite specific to the dog focus of ImageNet)
  circuit is the implementation of "oriented fur detectors" which detect fur parting, like hair on one's head.
  They're implemented by piecing together early fur detectors so that they converge in a particular way.
</p>

<figure style='image-rendering: pixelated;'>
  {images/Fur}
</figure>

<p>
Perhaps most frustratingly, it has many shape detectors that don't have a simple low-level articulation, but also are not yet very specific to a high-level feature.
</p>

<p>
With that said, there are many interesting features in <code>mixed3b</code> that we do understand quite well.
We'll start by examining those.
The "boundary detectors" are particularly worth paying attention to: these units use multiple cues (edge/curves, high-low frequency detectors, and color contrast) to detect the boundary of objects. Developing them seems to be an important goal of many early vision features.
</p>



{mixed3b_hipri}

<p>
  There are also a lot of other features, which don't fall into such a neat categorization.

</p>

{mixed3b_lowpri}


<!--
<h2 id="open">Open Questions about Early Vision Neuron Groups</h2>

<p>
  <b>Are these features to </b>
</p>

<p>
  <b>Is there a "periodic table of low-level visual features"?</b>
  By this we mean
</p>

-->

</d-article>
<d-appendix>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>
</html>
