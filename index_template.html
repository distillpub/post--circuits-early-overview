<html>
<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <meta charset="utf-8" />
  <link rel="stylesheet" type="text/css" href="index.css">
  <title>An Overview of Early Vision in InceptionV1</title>
  <script  type="text/javascript">
  function toggle_collapse() {{
    var className = event.target.parentNode.className;
    if (className.includes("collapsed")){{
      event.target.parentNode.className = className.replace("collapsed", "")
    }} else {{
      event.target.parentNode.className = className + " collapsed";
    }}
  }}
  </script>
</head>

<d-front-matter>
    <script type="text/json">{{
      "title": "",
      "description": "",
      "authors": [
      {{ "author": "Chris Olah", "authorURL": "https://colah.github.io", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
      {{ "author": "Nick Cammarata", "authorURL": "http://nickcammarata.com", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
      {{ "author": "Ludwig Schubert", "authorURL": "https://schubert.io/", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
      {{ "author": "Gabriel Goh", "authorURL": "http://gabgoh.github.io", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
      {{ "author": "Michael Petrov", "authorURL": "https://twitter.com/mpetrov", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }},
      {{ "author": "Shan Carter", "authorURL": "http://shancarter.com", "affiliation": "OpenAI", "affiliationURL": "https://openai.com"  }}
    ]
    }}</script>
  </d-front-matter>

<d-title>
<h1 style='font-size: 240%; grid-column-start: text; grid-column-end: page;'>An Overview of Early Vision in InceptionV1</h1>
<p style='font-size:  150%; line-height: 140%; margin-top: 10px;'>A guided tour of the first five layers of InceptionV1,<br> taxonomized into "neuron groups."</p>
</d-title>
<d-byline></d-byline>

<d-article>

  <section id="thread-nav" class="thread-info" style="grid-column-start: text; grid-column-end: text; margin-top: 20px; margin-bottom: 80px;">
      <div class="icon-multiple-pages"></div>
      <p class="explanation">
          This article is part of the <a style="border-bottom: none; color: #2e6db7; margin-left: 0px;" href="https://distill.pub/2020/circuits/">Circuits thread</a>, a collection of short articles and commentary by an open scientific collaboration delving into the inner workings of neural networks.<br>
          <!--<a style="border-bottom: none; color: #2e6db7; margin-left: 0px;">ðŸ”¬Learn how to get involved.</a>-->
          <!--<a class="overview" href="#">Thread Overview</a>-->
      </p>

      <a class="prev" href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a>
      <div class="next" href="#" style="color:#999;">Curve Detectors
        <div style="color:#BBB; font-size: 90%; line-height: 140%; margin-top: 4px;">Under discussion in <a href="http://slack.distill.pub/">Distill slack</a> <code>#circuits</code></div>
      </div>
  </section>


<p>
  The first few articles of the Circuits project will be focused on early vision in InceptionV1<d-cite bibtex-key="szegedy2015going"></d-cite>
  â€” for our purposes, the five convolutional layers leading up to the third pooling layer.
</p>

<figure class="l-body-outset">
  {images/Early}
  <figcaption style="margin-top:8px;">For our purposes, we'll consider early vision to be the first five layers. Click on a layer to jump to section.</figcaption>
</figure>

<p>
  Over the course of these layers, we see the network go from raw pixels
  up to sophisticated <a href="#group_mixed3b_boundary">boundary detection</a>, basic shape detection
  (eg. <a href="#group_mixed3b_curves">curves</a>, <a href="#group_mixed3b_circles_loops">circles</a>, <a href="#group_mixed3b_curve_shapes">spirals</a>, <a href="#group_mixed3a_angles">triangles</a>),
  <a href="#group_mixed3b_eyes">eye detectors</a>, and even crude detectors for <a href="#group_mixed3b_proto_head">very small heads</a>.
  Along the way, we see a variety of interesting intermediate features,
  including <a href="#conv2d1_discussion_complex_gabor">Complex Gabor detectors</a> (similar to some classic "complex cells" of neuroscience),
  <a href="#mixed3a_discussion_BW">black and white vs color detectors</a>, and <a href="#mixed3a_discussion_small_circle">small circle formation from curves</a>.
</p>

<p>
  Studying early vision has two major advantages as a starting point in our investigation.
  Firstly, it's particularly easy to study:
  it's close to the input, the circuits are only a few layers deep, there aren't that many different neurons,<d-footnote>It's common for vision models to have on the order of 64 channels in their initial convolutional layers, which are applied at many spatial positions. So while there are many neurons, the number of unique neurons is orders of magnitude smaller.</d-footnote> and the features seem quite simple.
  Secondly, early vision seems most likely to be universal: to have the same features and circuits form across different architectures and tasks.
</p>

<p>
  Before we dive into detailed explorations of different parts of early vision, we wanted to give a broader overview of how we presently understand it.
  This article sketches out our understanding, as an annotated collection of what we call "neuron groups."
  We also provide illustrations of selected circuits at each layer.
</p>

<p>
  By limiting ourselves to early vision, this article "only" considers the first 1,056 neurons of InceptionV1.<d-footnote>We will not discuss the "bottleneck" neurons in mixed3a/mixed3b, which we generally think of as low-rank connections to the previous layer.</d-footnote>
  But our experience is that a thousand neurons is more than enough to be disorienting when one begins studying a model.
  Our hope is that this article will help readers avoid this disorientation by providing some structure and handholds for thinking about them.
</p>



<h3 id="cards">Playing Cards with Neurons</h3>

<p>
  Dmitri Mendeleev is often accounted to have discovered the Periodic Table by playing "chemical solitaire," writing the details of each element on a card and patiently fiddling with different ways of classifying and organizing them.
  Some modern historians are skeptical about the cards, but Mendeleev's story is a compelling demonstration of that there can be a lot of value in simply organizing phenomena, even when you don't have a theory or firm justification for that organization yet.
  Mendeleev is far from unique in this.
  For example, in biology, taxonomies of species preceded genetics and the theory of evolution giving them a theoretical foundation.
</p>

<p>
  Our experience is that many neurons in vision models seem to fall into families of similar features.
  For example, it's not unusual to see a dozen neurons detecting the same feature in different orientations or colors.
  Perhaps even more strikingly, the same "neuron families" seem to recur across models!
  Of course, it's well known that Gabor filters and color contrast detectors reliably comprise neurons in the first layer of convolutional neural networks, but we were quite surprised to see this generalize to later layers.
</p>

<p>
  This article shares our working categorization of units in the first five layers of InceptionV1 into neuron families.
  These families are ad-hoc, human defined collections of features that seem to be similar in some way.
  We've found these helpful for communicating among ourselves and breaking the problem of understanding InceptionV1 into smaller chunks.
  While there are some families we suspect are "real", many others are categories of convenience, or categories we have low-confidence about.
  The main goal of these families is to help researchers orient themselves.
</p>

<p>
  In constructing this categorization, our understanding of individual neurons was developed by looking at feature visualizations, dataset examples, how a feature is built from the previous layer, how it is used by the next layer, and other analysis.
  It's worth noting that the level of attention we've given to individual neurons varies greatly: we've dedicated entire forthcoming articles to detailed analysis some of these units, while many others have only received a few minutes of cursory investigation.
</p>

<p>
  In some ways, our categorization of units is similar to <a href="http://netdissect.csail.mit.edu/">Net Dissect</a> <d-cite bibtex-key="bau2017network"></d-cite>, which correlates neurons with a pre-defined set of features and groups them into categories like color, texture, and object. 
  This has the advantage of removing subjectivity and being much more scalable. 
  At the same time, it also has downsides: correlation can be misleading and the pre-defined taxonomy may miss the true feature types.<d-footnote>
    <a href="http://netdissect.csail.mit.edu/">Net Dissect</a> <d-cite bibtex-key="bau2017network"></d-cite> was very elegant work which advanced our ability to systematically talk about network features.
    However, to understand the differences between correlating features with a pre-defined taxonomy and individually studying them, it may be illustrative to consider how it classifies some features.
    Net Dissect doesn't include the canonical InceptionV1, but it does include a variant of it.
    Glancing through their version of layer <a href="http://netdissect.csail.mit.edu/dissect/googlenet_imagenet/html/inception_3b-output.html"><code>mixed3b</code></a> we see many units which appear from dataset examples likely to be familiar feature types like curve detectors, divot detectors, boundary detectors, eye detector, and so forth, but are classified as weakly correlated with another feature â€” often objects that it seems unlikely could be detected at such an early layer.
    Or in another fun case, there is a feature (372) which is most correlated with a cat detector, but appears to be detecting left-oriented whiskers!
  </d-footnote>
  In particular, if we expect models to have novel, unanticipated features â€” for example, <a href="#group_mixed3a_high_low_frequency"></a>high-low frequency detectors</a> â€” the fact that they are unanticipated makes them impossible to include in a set of pre-defined features. 
  The only way to discover them is the laborious process of manually investigating each feature.
  In the future, you could imagine hybrid approaches, where a human investigator is saved time by having many features sorted into a (continually growing) set of known features, especially if the <a href="https://drafts.distill.pub/circuits-zoom-in/#claim-3">universality hypothesis</a> holds.
</p>

<h4>Caveats</h4>

<ul>
  <li>This is a broad overview and our understanding of many of these units is low-confidence. We fully expect, in retrospect, to realize we misunderstood some units and categories.</li>
  <li>Many neuron groups are catch-all categories or convenient organizational categories that we don't think reflect fundamental structure.</li>
  <li>Even for neuron groups we suspect do reflect a fundamental structure (eg. some can be recovered from factorizing the layer's weight matrices) the boundaries of these groups can be blurry and some neurons inclusion involve judgement calls.</li>
</ul>


<h3 id="presentation">Presentation of Neurons</h3>

<p>
  In order to talk about neurons, we need to somehow represent them.
  While we could use neuron indices, it's very hard to keep hundreds of numbers straight in one's head.
  Instead, we use <a href="https://distill.pub/2017/feature-visualization/">feature visualizations</a><d-cite key="erhan2009visualizing,olah2017feature,simonyan2013deep,nguyen2015deep,mordvintsev2015inceptionism,nguyen2016plug"></d-cite>, optimized images which highly stimulate a neuron.<d-footnote>
    Our feature visualization is done with the <a href="https://github.com/tensorflow/lucid">lucid library</a><d-cite key="olah2017feature"></d-cite>.
    We use small amounts of transformation robustness when visualizing the first few layers, because it has a larger proportional affect on their small receptive fields, and increase as we move to higher layers.
    For low layers, we use L2 regularization to push pixels towards gray.
    For the first layer, we follow the convention of other papers and just show the weights, which for the special case of the first layer are equivalent to feature visualization with the right L2 penalty.
  </d-footnote>
</p>
<p>
  When we represent a neuron with a feature visualization, we don't intend to claim that the feature visualization captures the entirety of the neuron's behavior.
  Rather, the role of a feature visualization is like a variable name in understanding a program.
  It replaces an arbitrary number with a more meaningful symbol <d-cite bibtex-key="olah2018building"></d-cite>.
</p>

<h3 id="presentation">Presentation of Circuits</h3>

<p>
  Although this article is focused on giving an overview of the features which exist in early vision, we're also interested in understanding how they're computed from earlier features. 
  To do this, we present <a href="https://distill.pub/2020/circuits/zoom-in/#claim-2">circuits</a> consisting of a neuron, the units it has the strongest (L2 norm) weights to in the previous layer, and the weights between them.<d-footnote>
    Some neurons in <code>mixed3a</code> and <code>mixed3b</code> are in branches consisting of a "bottleneck" 1x1 conv that greatly reduces the number of channels followed by a 5x5 conv. Although there is a ReLU between them, we generally think of them as a low rank factorization of a single weight matrix and visualize the product of the two weights. Additionally, some neurons in these layers are in a branch consisting of maxpooling followed by a 1x1 conv; we present these units as their weights replicated over the region of their maxpooling.
  </d-footnote>
  In some cases, we've also included a few neurons that have weaker connections if they seem to have particular pedagogical value; in these cases, we've mentioned doing so in the caption.
  Neurons are visually displayed by their feature visualizations, as discussed above.
  Weights are represented using a color map with red as positive and blue as negative.
</p>

<p>
  For example, here is a circuit of a circle detecting unit in <code>mixed3a</code> being assembled from earlier curves and a primitive circle detector.
  We'll discuss this example <a href="#mixed3a_discussion_small_circle">in more depth</a> later.
</p>

<figure style='grid-column-start: text-start; image-rendering: pixelated;'>
  {images/SmallCircle}
  <figcaption style="margin-top:12px;">Click on the feature visualization of any neuron to see more weights!</figcaption>
</figure>

<p>
  At any point, you can click on a neuron's feature visualization to see its weights to the 50 neurons in the previous layer it is most connected to (that is, how it assembled from the previous layer, and also the 50 neurons in the next layer it is most connected to (that is, how it is used going forward).
  This allows further investigation, and gives you an unbiased view of the weights if you're concerned about cherry-picking. 
</p>


<br><br>
<hr style="margin-top: 100px; margin-bottom: 100px;">
<br><br>

<h2 id="conv2d0"><code>conv2d0</code></h2>

<p>
  The first conv layer of every vision model we've looked at is mostly comprised of two kinds of features: color-contrast detectors and Gabor filters.
  InceptionV1's <code>conv2d0</code> is no exception to this rule, and most of its units fall into these categories.
</p>

<p>
  In contrast to other models, however, the features aren't perfect color contrast detectors and Gabor filters.
  For lack of a better word, they're messy.
  We have no way of knowing, but it seems likely this is a result of the gradient not reaching the early layers very well during training.
  Note that InceptionV1 predated the adoption of modern techniques like batch norm and Adam, which make it much easier to train deep models well.
  If we compare to the tf-slim rewrite of InceptionV1, which does use BatchNorm, we see crisper features.<d-footnote>
    The weights for the units in the first layer of the tf-slim version of InceptionV1, which adds BatchNorm. (Units are sorted by the first principal component of the adjacency matrix between the first and second layers.) These features are typical of a well trained conv net. Note how, unlike the canonical InceptionV1, these units have a crisp division between black and white Gabors, color Gabors, color-contrast units and color center-surround units. <br><br>
    <img src="images/slim_weights.png" style="width: 100%;">
  </d-footnote>
</p>

<p>
  One subtlety that's worth noting here is that Gabor filters almost always come in pairs of weights which are negative versions of each other, both in InceptionV1 and other vision models.
  A single Gabor filter can only detect edges at some offsets, but the negative version fills in holes, allowing for the formation of complex Gabor filters in the next layer.
</p>

{conv2d0}




<hr style="margin-top: 80px; margin-bottom: 100px;">
<br><br>

<h2 id="conv2d1"><code>conv2d1</code></h2>

<p>
  In <code>conv2d1</code>, we begin to see some of the classic <a href="https://en.wikipedia.org/wiki/Complex_cell">complex cell</a> features of visual neuroscience.
  These neurons respond to similar patterns to units in <code>conv2d0</code>, but are invariant to some changes in position and orientation.
</p>

<p style="margin-top: 20px;">
  <b id="conv2d1_discussion_complex_gabor">Complex Gabors:</b>
  A nice example of this is the "Complex Gabor" feature family.
  Like simple Gabor filters, complex Gabors detect edges.
  But unlike simple Gabors, they are relatively invariant to the exact position of the edge or which side is dark or light.
  This is achieved by being excited by multiple Gabor filters in similar orientations â€” and most critically, by being excited by "reciprocal Gabor filters" that detect the same pattern with dark and light switched.
  This can be seen as an early example of the "union over cases" motif.
</p>

<figure style='image-rendering: pixelated;'>
  {images/ComplexGabors}
  <figcaption style="margin-top: 12px;">All neurons in the previous layer with at least 30% of the max weight magnitude are shown,
    both <span class="positive-text">positive (excitation)</span> and <span class="negative-text">negative (inhibition)</span>.
    Click on a neuron to see its forwards and backwards weights.</figcaption>
</figure>

<p>
  Note that <code>conv2d1</code> is a 1x1 convolution, so there's only a single weight â€” a single line, in this diagram â€” between each channel in the previous and this one.<d-footnote>
    There is a pooling layer between them, so the features it connects to are pooled versions of the previous layer rather than original features.</d-footnote>
  This plays an important role in determining the features we observe: in models with larger convolutions in their second layer, we often see a jump to crude versions of the larger more complex features we'll see in the following layers.
</p>

<p>
  In addition to Complex Gabors, we see a variety of other features, including
  more invariant color contrast detectors, Gabor-like features that are less selective for a single orientation, and lower-frequency features.
</p>

{conv2d1}

<br>
<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="conv2d2"><code>conv2d2</code></h2>

<p>
In <code>conv2d2</code> we see the emergence of very simple shape predecessors.
This layer sees the first units that might be described as "line detectors", preferring a single longer line to a Gabor pattern and accounting for about 25% of units.
We also see tiny curve detectors, corner detectors, divergence detectors, and a single very tiny circle detector.
One fun aspect of these features is that you can see that they are assembled from Gabor detectors in the feature visualizations, with curves being built from small piecewise Gabor segments.
All of these units still moderately fire in response to incomplete versions of their feature, such as a small curve running tangent to the edge detector.
</p>

<p>
Since <code>conv2d2</code> is a 3x3 convolution, our understanding of these shape precursor features (and some texture features) maps to particular ways Gabor and lower-frequency edges are being spatially assembled into new features.
At a high-level, we see a few primary patterns:
</p>

<figure style='grid-column-start: page-start; image-rendering: pixelated;'>
  {images/GaborCombinations}
</figure>

<p>
  We also begin to see various kinds of texture and color detectors start to become a major constituent of the layer, including color-contrast and color center surround features, as well as Gabor-like, hatch, low-frequency and high-frequency textures.
  A handful of units look for different textures on different sides of their receptive field.
</p>

{conv2d2}





<br>
<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="mixed3a"><code>mixed3a</code></h2>

<p>
<code>mixed3a</code> has a significant increase in the diversity of features we observe.
Some of them  -- <a href="#group_mixed3a_curves">curve detectors</a> and <a href="#group_mixed3a_high_low_frequency">high-low frequency detectors</a> --
were discussed in <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In</a>
and will be discussed again in later articles in great detail.
But there are some really interesting circuits in <code>mixed3a</code> which we haven't discussed before,
and we'll go through a couple selected ones to give a flavor of what happens at this layer.
</p>


<p style="margin-top:20px;">
<b id="mixed3a_discussion_BW">Black & White Detectors:</b> One interesting property of <code>mixed3a</code> is the emergence of <a href="#group_mixed3a_bw_vs_color">"black and  white" detectors</a>, which detect the absence of color.
Prior to <code>mixed3a</code>, color contrast detectors look for transitions of a color to near complementary colors (eg. blue vs yellow).
From this layer on, however, we'll often see color detectors which compare a color to the absence of color.
Additionally, black and white detectors can allow the detection of greyscale images, which may be correlated with ImageNet categories (see {neuron/4a/479} which detects black and white portraits).
</p>
<p>
The circuit for our black and white detector is quite simple:
almost all of its large weights are negative, detecting the absence of colors.
  Roughly, it computes <code style="margin-left: 4px;"><b>NOT(</b>color_feature_1 <b>OR</b> color_feature_2 <b>OR</b> ...<b>)</b></code>.
</p>

<figure style='image-rendering: pixelated;'>
  {images/BW}
  <figcaption style="margin-top: 12px;">
    The sixteen strongest magnitude weights to the previous layer are shown.
    For simplicity, only one spatial weight for positive and negative have been shown, but they all have almost identical structure.
    Click on a neuron to see its forwards and backwards weights.</figcaption>
</figure>

<p style="margin-top:20px;">
<b id="mixed3a_discussion_small_circle">Small Circle Detector:</b> We also see somewhat more complex shapes in <code>mixed3a</code>. Of course, curves (which we discussed in <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In</a>) are a prominent example of this.
But there's lots of other interesting examples.
For instance, we see a variety of <a href="#group_mixed3a_eyes_small_circles">small circle and eye detectors</a> form
by piecing together <a href="#group_conv2d2_tiny_curves">early curves and circle detectors (<code>conv2d2</code>)</a>:
</p>

<figure style='grid-column-start: text-start; image-rendering: pixelated;'>
  {images/SmallCircle}
</figure>

<p style="margin-top:20px;">
  <b id="mixed3a_discussion_triangle">Triangle Detectors:</b> While on the matter of basic shapes, we also see <a href="#group_mixed3a_angles">triangle detectors</a>
  form from earlier <a href="#group_conv2d2_line">line (<code>conv2d2</code>)</a> and <a href="#group_conv2d2_shifted_line">shifted line (<code>conv2d2</code>)</a> detectors.
</p>

<figure>
  {images/Triangle}
  <figcaption style="margin-top: 12px;">
  The circuit  constructing a triangle detector.
  The choice of which neurons in the previous layer to show is slightly cherrypicked for pedagogy.
  The six neurons with the highest magnitude weights to the triangle are shown, plus one other neuron with slightly weaker weights.
  (Left leaning edges have slightly higher weights than right ones, but it seemed more illustrative to show two of both.) Click on neurons to see the full weights.</figcaption>
</figure>

<p>
  However, in practice, these triangle detectors (and other angle units)  seem to often just be used as multi-edge detectors downstream,
  or in conjunction with many other units to detect convex boundaries.
</p>

<p style="margin-top:20px;">
  The selected circuits discussed above only scratch the surface of the intricate structure in <code>mixed3a</code>.
  Below, we provide a taxonomized overview of all of them:
</p>

{mixed3a}




<br>
<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="mixed3b"><code>mixed3b</code></h2>

<p>
<code>mixed3b</code> straddles two levels of abstraction.
On the one hand, it has some quite sophisticated features that don't really seem like they should be characterized as "early" or "low-level": object boundary detectors, early head detectors, and more sophisticated part of shape detectors.
On the other hand, it also has many units that still feel quite low-level, such as color center-surround units.
</p>


<p style="margin-top:20px;">
  <b id="mixed3b_discussion_boundary">Boundary detectors:</b> One of the most striking transitions in <code>mixed3b</code> is the formation of boundary detectors.
  When you first look at the feature visualizations and dataset examples,
  you might think these are just another iteration of edge or curve detectors.
  But they are in fact combining a variety of cues to detect boundaries and transitions between objects.
  Perhaps the most important one is the high-low frequency detectors we saw develop at the previous layer.
  Notice that it largely doesn't care which direction the change in color or frequency is, just that there's a change.
</p>


<figure style='image-rendering: pixelated;'>
  {images/Boundary}
</figure>

<p>
  We sometimes find it useful to think about the "goal" of early vision.
  Gradient descent will only create features if they are useful for features in later layers.
  Which later features incentivized the creation of the features we see in early vision?
  These boundary detectors seem to be the "goal" of the <a href="#group_mixed3a_high_low_frequency">high-low frequency detectors (<code>mixed3a</code>)</a> we saw in the previous layer.
</p>

<p style="margin-top:20px;">
  <b id="mixed3b_discussion_curve_based">Curve-based Features:</b>
  Another major theme in this layer is the emergence of more complex and specific shape detectors based on curves.
  These include more <a href="">sophisticated curves</a>,
  <a href="#group_mixed3b_circles_loops">circles</a>, <a href="#group_mixed3b_curve_shapes">S-shapes</a>, <a href="#group_mixed3b_curve_shapes">spirals</a>,
  <a href="#group_mixed3b_divots">divots</a>, and <a href="#group_mixed3b_evolute">"evolutes"</a>
  (a term we've repurposed to describe units detecting curves facing away from the middle).
  We'll discuss these in detail in a forthcoming article on curve circuits, but they warrant mention here.
</p>

<p>
  Conceptually, you can think of the weights as piecing together curve detectors as something like this:
</p>

<figure style='image-rendering: pixelated;'>
  {images/CurveBased}
</figure>

<p style="margin-top:20px;">
  <b id="mixed3b_discussion_fur">Fur detectors:</b> Another interesting (albeit, probably quite specific to the dog focus of ImageNet)
  circuit is the implementation of <a href="#group_mixed3b_generic_oriented_fur">"oriented fur detectors"</a> which detect fur parting, like hair on one's head.
  They're implemented by piecing together <a href="#group_mixed3a_fur_precursors">fur precursors (<code>mixed3a</code>)</a> so that they converge in a particular way.
</p>

<figure style='image-rendering: pixelated;'>
  {images/Fur}
</figure>

<p>
  Again, these circuits only scratch the surface of <code>mixed3b</code>.
  Since it's a larger layer with lots of families, we'll go through a couple particularly interesting and well understood families first:
</p>

{mixed3b_hipri}

<p>
  In addition to the above features, are also a lot of other features which don't fall into such a neat categorization.
  One frustrating issue is that <code>mixed3b</code> has many units that don't have a simple low-level articulation, but also are not yet very specific to a high-level feature.
  For example, there are units which seem to be developing towards detecting certain animal body parts, but still respond to many other stimuli as well and so are difficult to describe.
</p>


{mixed3b_lowpri}



<hr style="margin-top: 20px; margin-bottom: 100px;">
<br><br>

<h2 id="conclusion">Conclusion</h2>

<p>
  The goal of this essay was to give a high-level overview of our present understanding of early vision in InceptionV1.
  Every single feature discussed in this article is a potential topic of deep investigation.
  For example, are curve detectors really curve detectors? What types of curves do they fire for? How do they behave on various edge cases? How are they built?
  Over the coming articles, we'll do deep dives rigorously investigating these questions for a few features, starting with curves.
</p>

<p>
  Our investigation into early vision has also left us with many broader open questions.
  To what extent do these feature families reflect fundamental clusters in features,
  versus a taxonomy that might be helpful to humans but is ultimately somewhat arbitrary?
  Is there a better taxonomy, or another way to understand the space of features?
  Why do features often seem to form in families?
  To what extent do the same features families form across models?
  Is there a "periodic table of low-level visual features", in some sense?
  To what extent do later features admit a similar taxonomy?
  We think these could be interesting questions for future work.
</p>


<!--
<h2 id="open">Open Questions about Early Vision Neuron Groups</h2>

<p>
  <b>Are these features to </b>
</p>

<p>
  <b>Is there a "periodic table of low-level visual features"?</b>
  By this we mean
</p>

-->

<br>
<div id="thread-nav" class="thread-info" style="margin-top: 40px;">
    <div class="icon-multiple-pages"></div>
    <p class="explanation">
        This article is part of the Circuits thread, a collection of short articles and commentary by an open scientific collaboration delving into the inner workings of neural networks.<br>
        <!--<a style="border-bottom: none; color: #2e6db7; margin-left: 0px;">ðŸ”¬Learn how to get involved.</a>-->
        <!--<a class="overview" href="#">Thread Overview</a>-->
    </p>

    <a class="prev" href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a>
    <div class="next" href="#" style="color:#999;">Curve Detectors
      <div style="color:#BBB; font-size: 90%; line-height: 140%; margin-top: 4px;">Under discussion in <a href="http://slack.distill.pub/">Distill slack</a> <code>#circuits</code></div>
    </div>
</div>

</d-article>
<d-appendix>
  <h3>Resources</h3>
  <p>
    Our taxonomy is available for download in the form of JSON files:
    <a href="layer_data/conv2d0.json">conv2d0</a>,
    <a href="layer_data/conv2d1.json">conv2d1</a>,
    <a href="layer_data/conv2d2.json">conv2d2</a>,
    <a href="layer_data/mixed3a.json">mixed3a</a>,
    <a href="layer_data/mixed3b.json">mixed3b</a>
  </p>
  <h3>Author Contributions</h3>
  <p>
    TODO
  </p>
  <h3>Acknowledgments</h3>
  <p>
    Thanks to Brice Menard, Sophia Sanborn, Nick Barry, Katarina Slama, Daniel Filan, Nick Moran, Joseph Abrahamson, Richard Meyes, David Valdman, Alex BÃ¤uerle, and Chris Hallacy.
  </p>
  
  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>
<d-bibliography src="bibliography.bib"></d-bibliography>

</html>
