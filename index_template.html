<html>
<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <link rel="stylesheet" type="text/css" href="index.css">
  <title>Neuron Groups</title>
</head>

<d-front-matter>
    <script type="text/json">{{
      "title": "",
      "description": "",
      "authors": [
        {{
          "author": "Authors TBD",
          "authorURL": "",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        }}
      ]
    }}</script>
  </d-front-matter>

<d-title>
<h1>An Overview of Early Vision</h1>
<p style='font-size:  170%;'>A guided tour of the first four layers of InceptionV1,<br> taxonomized into "neuron groups".</p>
</d-title>
<d-byline></d-byline>

<d-article>

<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#cards">Playing Cards with Neurons</a></div>
    <div>Layer Overviews</div>
    <ul>
      <li><a href="#conv2d0"><code>conv2d0</code></a></li>
      <li><a href="#conv2d1"><code>conv2d1</code></a></li>
      <li><a href="#conv2d2"><code>conv2d2</code></a></li>
      <li><a href="#mixed3a"><code>mixed3a</code></a></li>
      <li><a href="#mixed3b"><code>mixed3b</code></a></li>
    </ul>
    <div><a href="#open"><code>Open Questions</code></a></div>
  </nav>
</d-contents>

<div>
<p>
  The first few weeks of the Circuits project will be focused on early vision
  -- roughly, whatever goes on in the first 4-5 layers of InceptionV1.
  There's a few reasons we think early vision is a good starting point.
  Early vision seems particularly easy to study:
  it's close to the input, the circuits are only a few layers deep, there aren't that many different neurons, and the features seem quite simple.
  Early vision also seems most likely to be universal -- to be the same not just across different model architectures, but also across different tasks.
</p>

<p>
  Before we dive into details explorations of different parts of early vision, we wanted to give a broader overview of how we presently understand it.
  This article sketches out our understanding, as an annotated collection of what we call "neuron groups."
</p>
</div>


<h2 id="cards">Playing Cards with Neurons</h2>

<p>
  Dmitri Mendeleev is often accounted to have discovered the Periodic Table by playing "chemical solitaire," writing the details of each element on a card and patiently fiddling with different ways of classifying and organizing them.
  Some modern historians are skeptical about the cards, but Mendeleev's story is a compelling demonstration of that there can be a lot of value in simply organizing phenomena, even when you don't have a theory or firm justification for that organization yet.
  Mendeleev is far from unique in this.
  For example, in biology, taxonomies of species proceeded genetics and theory of evolution giving them a theoretical foundation.
</p>

<p>
  Our experience is that there are suspicious patterns in the neurons of vision models.
  It's not unusual to see a dozen neurons detecting the same feature in different orientations or colors.
  Perhaps even more strikingly, the same "neuron families" seem to recur across models!
  Of course, it's well known that Gabor filters and color contrast detectors reliably comprise neurons in the first layer of convolutional neural networks, but we were quite surprised to see this generalize to later layers.
</p>

<p>
  This article shares our working categorization of units in the first four layers of InceptionV1 into neuron families.
  We've found these helpful for communicating among ourselves and breaking the problem of understanding InceptionV1 into smaller chunks.
  While there are some families we suspect are "real", many others are categories of convenience, or categories we have low-confidence about.
</p>


<h2 id="conv2d0"><code>conv2d0</code></h2>

<p>
  The first conv layer of every vision model we've looked at is mostly comprised of two kinds of features: color-contrast detectors and Gabor filters.
  InceptionV1's <code>conv2d0</code> is no exception to this rule, and most of its units fall into these categories.
</p>

<p>
  With that said, it's features are surprisingly messy, compared to the
   perfect color contrast and Gabors we see in other models.
  Several neurons seem to be close to dead.
  We have no way of knowing, but the gradient probably wasn't reaching the early layers very well.
  Note that InceptionV1 predated the adoption of modern techniques like batch norm and Adam, which make it much easier to train deep models well.
</p>

{conv2d0}

<p>
  One subtlety that's worth noting here is that Gabor filters almost always come in pairs weights which are negative versions of each other, both in InceptionV1 and other vision models.
  A single Gabor filter can only detect edges at some offsets, but the negative version fills in holes, allowing for the formation of complex Gabor filters in the next layer.
</p>

<h2 id="conv2d1"><code>conv2d1</code></h2>

<p>
  In <code>conv2d1</code>, we begin to see some of the classic <a href="https://en.wikipedia.org/wiki/Complex_cell">complex cell</a> features of visual neuroscience.
  These neurons respond to similar patterns to units in <code>conv2d0</code>, but are invariant to some changes in position and orientation.
</p>

{conv2d1}


<h2 id="conv2d2"><code>conv2d2</code></h2>

<p>
In <code>conv2d2</code> we see the emergence of very simple shape predecessors, including tiny curve detectors, corner detectors, divergence detectors, and a single very tiny circle detector.
One fun aspect of these features is that you can see that they are assembled from Gabor detectors in the feature visualizations, with curves being built from small piecewise Gabor segments.
This layer also sees the first units that might be described as "line detectors", preferring a single longer line to a Gabor pattern.
All of these units still moderately fire in response to incomplete versions of their feature, such as a small curve running tangent to the edge detector.
</p>

<p>
Various kinds of texture detector also start to become a major constituent of the layer, including Gabor-like, hatch, low-frequency and high-frequency textures.
In addition to these, we see a few units that detect different textures on different sides of their receptive field.
</p>

{conv2d2}



<h2 id="mixed3a"><code>mixed3a</code></h2>

<p>
In <code>mixed3a</code>
</p>


{mixed3a}



<h2 id="mixed3b"><code>mixed3b</code></h2>

<p>
<code>mixed3b</code> is an awkward layer, straddling a boundary between two levels of abstraction.
On the one hand, it has some quite sophisticated features that don't really seem like they should be characterized as "early" or "low-level": object boundary detectors, early head detectors, more sophisticated part of shape detectors.
On the other hand, it also has many units that still feel quite low-level, such as color center-surround units.
Perhaps most frustratingly, it has many shape detectors that don't have a simple low-level articulation, but also are not yet very specific to a high-level feature.
</p>

<p>
With that said, there are many interesting features in <code>mixed3b</code> that we do understand quite well.
We'll start by examining those.
The "boundary detectors" are particularly worth paying attention to: these units use multiple cues (edge/curves, high-low frequency detectors, and color contrast) to detect the boundary of objects. Developing them seems to be an important goal of many early vision features
</p>


{mixed3b_hipri}

<p>
  There are also a lot of other features, which don't fall into such a neat categorization.

</p>

{mixed3b_lowpri}

<h2 id="open">Open Questions about Early Vision Neuron Groups</h2>

<p></p>

<p></p>

</d-article>
</html>
